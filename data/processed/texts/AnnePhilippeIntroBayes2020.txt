Introduction Ã  la statistique bayÃ©sienne Anne Philippe  UniversitÃ© de Nantes, LMJL 2020 
1.ModÃ©lisation bayÃ©sienne  2.Estimation  et prÃ©vision bayÃ©sienne 3.Construition des lois a priori plan du cours 
Chapitre 1  ModÃ©lisation bayÃ©sienne  DÃ©finitions et exemples 
Exemple  et  rappels sur le calcul des lois conditionnelles  Exemple du billard
On lance une bille qui sâ€™arrÃªte Ã  un point  uniformÃ©ment distribuÃ©.   
Question comment dÃ©terminer la valeur de  sans effectuer de mesures  ?  
On repÃ¨te la mÃªme expÃ©rience N fois de  faÃ§on indÃ©pendante et on note X le nombre de fois oÃ¹ elle sâ€™arrÃªte Ã  gauche du point dâ€™arrÃªt 
Comment estimer  Ã  partir de X ? Î¸âˆˆ[0,1]Î¸Î¸
Approche frÃ©quentiste   
 suit une loi binomiale  oÃ¹  est un paramÃ¨tre inconnu  
La vraisemblance sâ€™Ã©crit   
lâ€™estimateur du MV est  Ã©gal Ã      
Cet estimateur nâ€™utilise pas  la 1er expÃ©rience alÃ©atoire. Xâ„¬(N,Î¸)Î¸V(Î¸,X)=(NX)Î¸X(1âˆ’Î¸)Nâˆ’XÌ‚Î¸MVN=XN
 est une v.a. distribuÃ©e  
la loi de   est  la loi uniforme sur [0,1]  
la  loi binomiale   est la loi conditionnelle  de X sachant  
Quelle est la loi de  sachant X ? Î¸Î¸â„¬(N,Î¸)Î¸Î¸Alternative 
Conditionnement Soit (X,Y) un couple de variables alÃ©atoires discrÃ¨tes.  la loi marginale de Y  sâ€™Ã©crit    La loi conditionnelle de X sachant Y  est donnÃ©e par la formule de Bayes  :  P(Y=y)=âˆ‘xP(X=x,Y=y)P(X=x|Y=y)=P(X=x,Y=y)P(Y=y) Conditionnement Soit (X,Y) un couple de variables alÃ©atoires continues.  On note  la densitÃ© du couple (X,Y).  la loi marginale de Y admet une densitÃ© Ã©gale Ã    La loi conditionnelle de X sachant Y admet une densitÃ© dÃ©finie par   ffY(y)=âˆ«f(x,y)dxfX|Y(x|y)=f(x,y)fY(y)
Formule de BAYESSoit (X,Y) un couple de variables alÃ©atoires  La formule de Bayes est donnÃ©e par   Câ€™est lâ€™outil central pour calculer les lois conditionnelles. P(XâˆˆA,YâˆˆB)=âˆ«BP(XâˆˆA|Y=y)fY(y)dysiÂ YÂ estÂ continueâˆ‘yâˆˆBP(XâˆˆA|Y=y)P(Y=y)siÂ YÂ estÂ discrÃ¨te
Exemple du billard (cont.)  
On connait    et  on note  sa densitÃ© :    la loi conditionnelle  
Formule de BAYES :     Î¸âˆ¼U(0,1)Ï€(Ï‘)=I[0,1](Ï‘)P(X=x|Î¸=Ï‘)=(Nx)Ï‘x(1âˆ’Ï‘)Nâˆ’xP(X=x,Î¸âˆˆB)=âˆ«BP(X=x|Î¸=Ï‘)Ï€(Ï‘)dÏ‘=âˆ«B(Nx)Ï‘x(1âˆ’Ï‘)Nâˆ’xI[0,1](Ï‘)dÏ‘P(X=x,Î¸âˆˆB)=P(Î¸âˆˆB|X=x)P(X=x)
Exemple du billard (cont.)  
La loi marginale de X est    
En appliquant la formule  de Bayes, on obtient P(X=x)=P(X=x,Î¸âˆˆ[0,1])=(Nx)âˆ«10Ï‘x(1âˆ’Ï‘)Nâˆ’xdÏ‘  
câ€™est la loi beta de paramÃ¨tres 
(x+1 , N-x+1) 
Ï€
(
Ï‘
|
X
=
x
)
=
Ï‘
x
(
1
âˆ’
Ï‘
)
N
âˆ’
x
âˆ«
1
0
Ï‘
x
(
1
âˆ’
Ï‘
)
N
âˆ’
x
d
Ï‘
P
(
Î¸
âˆˆ
B
|
X
=
x
)
=
P
(
X
=
x
,
Î¸
âˆˆ
B
)
P
(
X
=
x
)
=
âˆ«
B
Ï‘
x
(
1
âˆ’
Ï‘
)
N
âˆ’
x
d
Ï‘
âˆ«
1
0
Ï‘
x
(
1
âˆ’
Ï‘
)
N
âˆ’
x
d
Ï‘
=
âˆ«
B
Ï€
(
Ï‘
|
X
=
x
)
d
Ï‘ expÃ©riences numÃ©riques N 
5
10
50
100
X 
3
5
31
68
MV
 0.6    
0.5
0.62
0.68
Loi 
mean 
quantile 
2.5% 
quantile 
97.5 % 
(prior)  
.50
.O3
.98
posteriori
N = 5
.57
.22
.88
N = 1O
.50
.24
.77
N = 50
.62 
.48
.74
N=100
.68 
.58
.76Ï€Ï€(.|X)Representation des lois conditionnelles  de  sachant les N observations Î¸
DÃ©finition du modÃ¨le bayÃ©sien loi a priori 
ModÃ¨le paramÃ©trique bayÃ©sien
On considÃ¨re   une famille de lois indexÃ©es par .  On note   les observations.  
On suppose que  est une VARIABLE ALÃ‰ATOIRE 
la loi   est interprÃ©tÃ© comme  la loi conditionnelle de  sachant le paramÃ¨tre PnÎ¸(â‹…),Î¸âˆˆÎ˜Î¸X=(X1,...,Xn)Î¸PnÎ¸(â‹…)X=(X1,...,Xn)Î¸
PnÎ¸(â‹…)=Pn(â‹…|Î¸)
cette loi  est construite Ã  partir des informations disponibles sur  le paramÃ¨tre  avant de collecter des donnÃ©es  
L'information (dite a priori ) provient  
dâ€™avis dâ€™expert  
de rÃ©sultats dâ€™experiences prÃ©cÃ©dentes dont les rÃ©sultats sont supposÃ© similaires  
des propriÃ©tÃ©s physiques   Î¸
DÃ©finition :   la loi du paramÃ¨tre  est appelÃ©e  Â«Â loi a prioriÂ Â» Î¸
Exemple du billard :  On sait que  est la position du point dâ€™arrÃªt de la premiÃ¨re boule. Dâ€™aprÃ¨s lâ€™expert ce point est uniformÃ©ment  distribuÃ©  Î¸ FrÃ©quentiste  vs   BayÃ©sien  
 observations 
il existe  inconnu   
 est la loi des observations X1,...,XnÎ¸OâˆˆÎ˜PnÎ¸0(â‹…)
 observations  
 variable alÃ©atoire  
 est la loi conditionnelle des observations sachant  X1,...,XnÎ¸PnÎ¸(â‹…)=P(n)(â‹…|Î¸)Î¸
InfÃ©rence estimateur    intervalle de confiance Ì‚Î¸(X1,...,Xn)Pâˆ’psâˆ’LpÎ¸0?
InfÃ©rence loi a posteriori 
InfÃ©rence 
Lâ€™objectif est de mettre Ã  jour la loi a priori  sur  Ã  partir  des observations.  
En combinant la loi a priori et la loi des observations sachant le paramÃ¨tre , on peut calculer  
La loi  jointe de    
La loi marginale de   appelÃ©e loi prÃ©dictive a priori  
La loi conditionnelle de  sachant  Î¸Î¸(X1,...,Xn,Î¸)(X1,...,Xn)Î¸(X1,...,Xn)
DÃ©finition :  
la loi conditionnelle du paramÃ¨tre 
 sachant les 
observations est appelÃ©e  Â«Â loi a posterioriÂ Â» 
Î¸
calcul  de la loi a posteriori continue/continue
La loi jointe   :  
La loi marginale :  
La loi a posteriori : gn(x1,...,xn,Ï‘)=Ï€(Ï‘)f(n)(x1,...,xn|Ï‘)mn(x1,...,xn,)=âˆ«Î˜Ï€(Ï‘)f(n)(x1,...,xn|Ï‘)dÏ‘Ï€(Ï‘|x1,...,xn,)=Ï€(Ï‘)f(n)(x1,...,xn|Ï‘)mn(x1,...,xn)
   :  densitÃ© de la loi a priori   
  :  densitÃ© la loi de  sachant Ï€f(n)(x1,...,xn|Î¸)(X1,...,Xn)Î¸ calcul de la loi a posteriori  :  discrÃ¨te / discrÃ¨te
La loi marginale :  
La loi a posteriori : P(n)(X1=x1,...,Xn=xn)=pâˆ‘1Ï€iP(n)(X1=x1,...,Xn=xn|Î¸=Î¸i)P(Î¸=Î¸i|X1=x1,...,Xn=xn)=Ï€iP(n)(X1=x1,...,Xn=xn|Î¸=Î¸i)P(n)(X1=x1,...,Xn=xn)
   la loi a priori   est dÃ©finie par  
  : loi de  sachant  Î˜={Î¸1,...,Î¸p}Ï€i=P(Î¸=Î¸i)P(n)(X1=x1,...,Xn=xn|Î¸)(X1,...,Xn)Î¸
calcul de la loi a posteriori  :  continue/discrÃ¨te
La loi marginale :  
La loi a posteriori : Pn(X1=x1,...,Xn=xn)=âˆ«Î˜Ï€(Ï‘)P(n)(X1=x1,...,Xn=xn|Ï‘)dÏ‘Ï€(Ï‘|x1,...,xn,)=Ï€(Ï‘)P(n)(X1=x1,...,Xn=xn|Ï‘)Pn(X1=x1,...,Xn=xn)
   :  densitÃ© de la loi a priori   
la loi de  sachant  est discrÃ¨te Ï€(X1,...,Xn)Î¸
ğŸ“
calcul de la loi a posteriori  :  discrÃ¨te / continue
La loi marginale :  
La loi a posteriori : mn(x1,...,xn)=pâˆ‘1Ï€if(n)(x1,...,xn|Î¸i)P(Î¸=Î¸i|X1,...,Xn)=Ï€if(n)(X1,...,Xn|Î¸i)mn(X1,...,Xn)
   la loi a priori   est dÃ©finie par  
  :  densitÃ© la loi de  sachant  Î˜={Î¸1,...,Î¸p}Ï€i=P(Î¸=Î¸i)f(n)(x1,...,xn|Î¸)(X1,...,Xn)Î¸
ğŸ“
A partir de la loi a priori et la conditionnelle de X sachant  on connait la loi a posteriori Ã  une constante multiplicative  prÃ¨s.  
par exemple si  est une va continue on a   ou   
Comme on cherche une loi de probabilitÃ©,  la loi a posteriori est bien dÃ©finie Ã  partir des expressions de droiteÎ¸Î¸Ï€(Ï‘|x1,...,xn,)âˆÏ€(Ï‘)f(n)(x1,...,xn|Ï‘)Ï€(Ï‘|x1,...,xn,)âˆÏ€(Ï‘)P(n)(X1=x1,...,Xn=xn|Ï‘)Remarque   Chapitre 2  Estimation  et prÃ©vision  bayÃ©sienne 
Estimation bayÃ©sienne  = estimation probabiliste
a partir du modÃ¨le bayÃ©sien,  on obtient une loi de probabilitÃ© sur le paramÃ¨tre  : la loi a posteriori  
cette loi rÃ©sume lâ€™information provenant des donnÃ©es  et de lâ€™information a priori  X=(X1,...,Xn)
Loi a posteriori
Loi a priori
Observations 
Estimation par intervallecrÃ©dibilitÃ©
intervalle de crÃ©dibilitÃ© 
On fixe .  on cherche un intervalle   tel que  Î±âˆˆ]0,1/2[[l(X);u(X)]âŠ‚Î˜P(Î¸âˆˆ[l(X);u(X)]|X)=1âˆ’Î±
InterprÃ©tation : Ayant observÃ© X, lâ€™intervalle contient le paramÃ¨tre  avec une probabilitÃ©   
exemple :  Î¸1âˆ’Î±l(X)=q(Î±/2,X)etu(X)=q(1âˆ’Î±/2,X)oÃ¹  quantile dâ€™ordre  de la loi a posteriori q(Î±,X)Î± intervalles de crÃ©dibilitÃ© 
Il nâ€™y a pas unicitÃ© des lssâ€™intervalle,  
 tous les  intervalles de la forme suivante sont des intervalles de crÃ©dibilitÃ©    avec [q(Î²,X);q(1âˆ’Î±+Î²,X)]0â‰¤Î²â‰¤Î±
On cherche le plus court intervalle :  on cherche la valeur de  qui minimise la longueur  Î²q(1âˆ’Î±+Î²,X)âˆ’q(Î²,X)
dÃ©fauts des IC
  vÃ©rifie   est une rÃ©gion de niveau   plus courte que IC optimal [a,b]âŠ‚[l(X),u(X)]P(Î¸âˆˆ[a,b]|X1,...,Xn)â‰ˆ0â‡’[l(X),a]âˆª[b,u(X)]1âˆ’Î±2.La gÃ©nÃ©ralisation en dimension supÃ©rieure est difficile 1. Soit   le plus court  intervalle de crÃ©dibilitÃ© de niveau [l(X),u(X)]1âˆ’Î±
RÃ©gion Highest Posterior Density (HPD)
On cherche la plus petite rÃ©gion   telle que   
ces rÃ©gions sont de la forme         
On choisit  la  valeur    telle que   HâŠ‚Î˜P(Î¸âˆˆH|X1,...,Xn)=1âˆ’Î±H(K)={Î¸:Ï€(Î¸|X1,...,Xn)>K}K:=K1âˆ’Î±(X)P(Î¸âˆˆH(K1âˆ’Î±(X))|X1,...,Xn)=1âˆ’Î±
La rÃ©gion  
est appelÃ© rÃ©gion HPD de niveau 
{
Î¸
:
Ï€
(
Î¸
|
X
1
,
.
.
.
,
X
n
)
>
K
1
âˆ’
Î±
(
X
)
}
1
âˆ’
Î±
PropriÃ©tÃ©s 
la dÃ©finition est indÃ©pendante de la dimension de  
En dimension 1 si la distribution est unimodale alors la rÃ©gion HPD est un intervalle qui coincide avec le plus court intervalle de crÃ©dibilitÃ© 
la dÃ©finition se gÃ©nÃ©ralise aux lois discrÃ¨tes en prenant  Î˜H(K)={Ï‘:P(Î¸=Ï‘|X1,...,Xn)>K} Exemple
On modÃ©lise   le nombre de pannes par une loi de poisson de paramÃ¨tre  > 0  
La loi priori est la loi  exponentielle de paramÃ¨tre 1 
Calcul de la loi a posteriori  
 
 
 
On observe  
loi uniondale : IC et HPD coÃ¯ncident X1,...,XnÎ¸P(X1=x1,...,Xnxn|Ï‘)=eâˆ’nÏ‘Ï‘âˆ‘xi1x1!...xn!Ï€(Ï‘|)âˆeâˆ’Ï‘IÏ‘>0Ï€(Ï‘|x1,...,xn)âˆeâˆ’(n+1)Ï‘Ï‘âˆ‘xiIÏ‘>0n=10Â etÂ âˆ‘Xi=20loi gamma (21,11) 
ğŸ“
(1) on  calcule et represente la longueur en fonction de   (2) on dÃ©termine la valeur de  qui minimise la longueur  (3) on en dÃ©duit le plus court intervalle de crÃ©dibilitÃ© qui coÃ¯ncide avec la rÃ©gion HPDÎ²Î²
En utilisant R  
qgamma  quantile de la loi gamma 
which.min recherche du min
lien avec les intervalles de confiance. 
un intervalle de confiance de niveau  est une intervalle alÃ©atoire     [a(X) , b(X)] tel que,  pour tout  :        % des intervalles de confiance contiennent la vraie valeur du paramÃ¨tre  
un intervalle de crÃ©dibilitÃ© de niveau  est une intervalle  [l(X) u(X)] tel que   Ayant observÃ© X,  le paramÃ¨tre appartient Ã  lâ€™intervalle avec une probabilitÃ© 1âˆ’Î±Î¸PÎ¸([a(X),b(X)]âˆ‹Î¸)=1âˆ’Î±1âˆ’Î±1âˆ’Î±P(Î¸âˆˆ[l(x),u(X)]|X)=1âˆ’Î±1âˆ’Î±
ProbabilitÃ© frÃ©quentiste dâ€™un intervalle de crÃ©dibilitÃ©
Soit [l(X) u(X)]  un intervalle de crÃ©dibilitÃ© de niveau   
Pour le modÃ¨le , sa probabilitÃ© frÃ©quentiste est Ã©gale Ã   
  En gÃ©nÃ©ral     1âˆ’Î±{PÎ¸,Î¸âˆˆÎ˜}PÎ¸([l(X),u(X)]âˆ‹Î¸)=Î²(Î¸,n)
Î²(Î¸,n)â‰ 1âˆ’Î± Exemple  : le  modÃ¨le exponentiel  -conditionnellement Ã  ,   iid suivant une loi exponentielle  > 0  - La loi priori est la gamma de paramÃ¨tre (a,b)  1.La loi a posteriori  est la loi gamma  2.Pour tout  : est un intervalle de crÃ©dibilitÃ© de niveau  Î¸X1,...,XnÎ¸(a+n,b+âˆ‘Xi)Î²âˆˆ(0,Î±)[gn+a(Î²)b+âˆ‘Xi;gn+a(1âˆ’Î±+Î²)b+âˆ‘Xi]1âˆ’Î±
ğŸ“
Notation :  
 est le quantile dâ€™ordre 
   et 
 la 
fonction de rÃ©partition de la loi gamma (a, 1)  
g
a
(
Î²
)
Î²
G
a
le niveau frÃ©quentiste est   
A n fixÃ©, quand a-> 0 et b-> 0 alors le niveau frÃ©quentiste converge vers  
Approximation de la loi  gamma  
Quand , on a    
Si   avec xâ‰ 0  alors  quand  
A (a,b) fixÃ©s,  le niveau frÃ©quentiste converge vers  quand .   Lâ€™intervalle de crÃ©dibilitÃ© de niveau  est un intervalle de confiance asymptotiquement de niveau  Î²(n,Î¸)=Gn(gn+a(1âˆ’Î±+Î²)âˆ’bÎ¸)âˆ’Gn(gn+a(Î²)âˆ’bÎ¸)1âˆ’Î±nâ†’âˆgn(Î²)âˆ¼qÎ²n+nxnâˆ¼nx+nGn(xn)âˆ¼Î¦(x)nâ†’âˆ1âˆ’Î±nâ†’âˆ1âˆ’Î±1âˆ’Î±
Notation :  
 est le quantile 
dâ€™ordre 
   et 
 la fonction de 
rÃ©partition de la loi gaussienne 
N(0,1) 
q
Î²
Î²
Î¦
ğŸ“
CrÃ©dibilitÃ© dâ€™une hypothÃ¨se 
On considÃ¨re le test statistique                         
On suppose que  pour i=1,2 
RÃ¨gle de dÃ©cision BayÃ©sienne :  
On calcule les probabilitÃ©s a posteriori  des hypothÃ¨ses :   pour i=1,2 
On dit que  est plus crÃ©dible que  si    
La valeur de la probabilitÃ© a posteriori quantifie la crÃ©dibilitÃ© de lâ€™hypothÃ¨se. H0:Î¸âˆˆÎ˜0contreÂ H1:Î¸âˆˆÎ˜1P(Î¸âˆˆÎ˜i)â‰ 0P(Î¸âˆˆÎ˜i|X)H0H1P(Î¸âˆˆÎ˜0|X)â‰¥P(Î¸âˆˆÎ˜1|X)
Exemple : modÃ¨le exponentiel (cont.) 
La loi a posteriori est la loi  gamma  
On prend  a=b= 1  
On veut tester  contre   
On Ã©value la pobabilititÃ© a posteriori de lâ€™hypothÃ¨se nulle, câ€™est Ã  dire                         ( = 83 %) (n+a,b+âˆ‘Xi)Î¸â‰¤1Î¸>1P(Î¸â‰¤1|X)=Gn+a(b+âˆ‘Xi)
 PrÃ©vision BayÃ©sienne PrÃ©vision en loi
Loi prÃ©dictive 
 Objectif : on veut prÃ©voir Â   Ã  partir des observations passÃ©es  dans le modÃ¨le bayesien Xn+1(X1,...,Xn)
Loi a priori 
Loi conditionnelle 
des observations 
sachant le paramÃ¨tre 
Loi a posteriori 
Loi 
prÃ©dictive  
DÃ©finition : la loi prÃ©dictive est la loi 
conditionnelle de 
 sachant  
X
n
+
1
(
X
1
,
.
.
.
,
X
n
)
.
Calcul de la loi prÃ©dictive 
La loi prÃ©dictive sâ€™Ã©crit :   
OÃ¹   est la densitÃ© de la  loi conditionnelle de   sachant  et le passÃ©  :   p(xn+1|x1,...,xn)=âˆ«Î˜p(xn+1|Ï‘,x1,...,xn,)Ï€(Ï‘|x1,...,xn)dÎ¸Â continueÂ âˆ‘Ï‘âˆˆÎ˜p(xn+1|Ï‘,x1,...,xn,)P(Î¸=Ï‘|x1,...,xn)Â discrÃ¨teÂ p(xn+1|Ï‘,x1,...,xn)Xn+1Î¸X1,...,Xnp(xn+1|Ï‘,x1,...,xn)=f(x1,...,xn+1|Ï‘)f(x1,...,xn|Ï‘)
La loi prÃ©dictive est un mÃ©lange de loi.  
ğŸ“
A partir de la loi prÃ©dictive on construit  1. Un intervalle de prÃ©vision de niveau , câ€™est un intervalle  tel que  2. Un prÃ©dicteur ponctuel : Pour lâ€™erreur , la meilleure approximation de  par une fonction de  est lâ€™espÃ©rance conditionnelle  1âˆ’Î±[P1;P2]P(Xn+1âˆˆ[P1;P2]|X1,...,Xn)=1âˆ’Î±L2Xn+1X1,...,XnÌ‚Xn+1=E(Xn+1|X1,...,Xn) Exemple : observations iid suivant  f(â‹…|Î¸)
Lâ€™indÃ©pendance conditionnellement Ã   implique que   
Dâ€™ou la loi prÃ©dictive  
Le prÃ©dicateur ponctuel est  Î¸p(xn+1|Ï‘,x1,...,xn)=f(xn+1|Ï‘)p(xn+1|x1,...,xn)=âˆ«Î˜f(xn+1|Ï‘)Ï€(Ï‘|x1,...,xn)dÏ‘Ì‚Xn+1=âˆ«Î˜E(Xn+1|Ï‘)Ï€(Ï‘|X1,...,Xn)dÏ‘
ğŸ“
Exemple : regression linÃ©aire suivant  X=Î¸t+ÏµÏµÂ iidÂ N(0,Ïƒ2)
Lâ€™indÃ©pendance conditionnellement Ã   implique que     câ€™est la densitÃ© de la loi gaussienne  de moyenne  et de variance    
Dâ€™ou la loi prÃ©dictive 
 
Le prÃ©dicteur ponctuel est Ã©gal Ã   Î¸p(xn+1|Ï‘,x1,...,xn)=f(xn+1|Ï‘)Î¸tn+1Ïƒ2p(xn+1|x1,...,xn)=âˆ«Î˜Ã—R+12Ï€Ïƒe12Ïƒ2(xn+1âˆ’Î¸tn+1)2Ï€(Ïƒ2,Ï‘|x1,...,xn)dÏ‘dÏƒ2Ì‚Xn+1=E(Î¸|X1,...,Xn)tn+1
ğŸ“
Estimateurs de Bayes Construction dâ€™estimateurs Ã  partir de la loi a posteriori
Estimateurs de Bayes 
Soit  L une fonction de coÃ»t :  elle permet Ã©valuer la qualitÃ© dâ€™un estimateur    Exemple :   plus gÃ©nÃ©ralement câ€™est une fonction  positive  L:  telle que   
Le risque bayesien dâ€™un estimateur  est  Î´:=Î´(X)Â duÂ paramÃ¨treÂ Î¸L2(Î´,Î¸)=(Î´âˆ’Î¸)2Â coÃ»t/erreurÂ quadratiqueÂ L2L1(Î´,Î¸)=|Î´âˆ’Î¸|Â coÃ»t/erreurÂ absolueÂ L1Î˜Ã—Î˜â†’R+L(Î´;Î¸)=0â‡”Î´=Î¸Î´Â duÂ paramÃ¨treÂ Î¸r(Î´,Ï€)=E(L(Î´(X),Î¸))=âˆ«L(Î´(x),Ï‘)gn(x,Ï‘)dx1...dxndÏ‘=âˆ«L(Î´(x),Ï‘)Ï€(Ï‘|x)dÏ‘mn(x1,...,xn)dx1...dxn
DÃ©finition : Un estimateur  est un estimateur de Bayes sous coÃ»t L, sâ€™il minimise le risque bayesien  câ€™est Ã  dire  pour estimateur Î´Ï€r(Î´Ï€,Ï€)â‰¤r(Î´,Ï€)Î´ Construction des estimateurs de Bayes 
On dÃ©finit    
ThÃ©orÃ¨me :   est un estimateur de Bayes  
Cas particulier  
CoÃ»t quadratique :  lâ€™estimateur de Bayes est lâ€™espÃ©rance de la loi a posteriori        
CoÃ»t absolue : lâ€™estimateur de Bayes est la mÃ©diane  de la loi a posterioriÏ(Î´)=âˆ«Î˜L(Î´,Ï‘)Ï€(Ï‘|X1,...Xn)dÏ‘Î´Ï€(X)=argminÎ´Ï(Î´)Î´Ï€(X)=E(Î¸|X1,...Xn)
ğŸ“
ğŸ“
PropriÃ©tÃ©s asymptotiques â€”
Contexte 
on suppose que le modÃ¨le  est rÃ©gulier  
Sous ces hypothÃ¨se :  lâ€™estimateur du maximum de vraisemblance converge presque surement et   est asymptotiquement efficace  
On suppose que les observations sont iid suivant  oÃ¹  appartient Ã  lâ€™intÃ©rieur de  
ThÃ©orÃ¨me  1  si  sont deux lois a priori telles que   alors pour tout  :  {PnÎ¸=Pn(â‹…|Î¸)Î¸âˆˆÎ˜}Î¸MVnÎ¸MVnfÎ¸0Î¸0Î˜Ï€1,Ï€2Ï€i(Î¸)>0,âˆ€Î¸âˆˆÎ˜AâŠ‚Î˜âˆ«A|Ï€1(Ï‘|X)âˆ’Ï€2(Ï‘|X)|dÏ‘p.s.nâ†’âˆ0
ThÃ©orÃ¨me 2 Si  est une loi a  priori telle que   et  est  sur  alors  1) Pour tout intervalle ouvert  U contenant      on a  2)  3) Ï€Ï€(Î¸)>0,âˆ€Î¸âˆˆÎ˜Ï€C1Î˜Î¸0P(Î¸âˆˆU|X)psnâ†’âˆ1E(Î¸|X)Pnâ†’âˆÎ¸0Var(Î¸|X)Pnâ†’âˆ0 ThÃ©orÃ¨me 3 Si  est une loi a  priori telle que   et  est  sur  alors  1) On peut approcher la loi a posteriori par la loi gaussienne de moyenne  et de variance  2) On peut approcher la loi a posteriori par la loi gaussienne de moyenne  et de variance   oÃ¹ I est lâ€™information de Fisher apportÃ©e par une observation  3) On a Ï€Ï€(Î¸)>0,âˆ€Î¸âˆˆÎ˜Ï€C2Î˜E(Î¸|X)Var(Î¸|X)Î¸MVnnâˆ’1Iâˆ’1(Î¸MVn)n(E(Î¸|X)âˆ’Î¸MVn)nâ†’âˆP0
ConsÃ©quence du thÃ©orÃ¨me 3  1) lâ€™estimateur de Bayes sous coÃ»t quadratique  câ€™est Ã  dire   est asymptotiquement efficace  4) Le niveau frÃ©quentiste des intervalles de crÃ©dibilitÃ© de niveau  converge vers  Î´Ï€(X)=E(Î¸|X1,...Xn)1âˆ’Î±1âˆ’Î±Î²(Î¸,n)nâ†’âˆ1âˆ’Î±On applique le thÃ©orÃ¨me de SLutski   Le premier terme converge vers 0 en proba et le confond en loi vers n(E(Î¸|X)âˆ’Î¸0)=n(E(Î¸|X)âˆ’Î¸MVn)+n(Î¸MVnâˆ’Î¸0)N(0,Iâˆ’1(Î¸0))
Chapitre 3   Loi a priori  
Lois informatives   Loi discrÃ¨te  
On suppose que  le paramÃ¨tre appartient Ã  un ensemble fini  avec les  probabilitÃ©s   câ€™est Ã  dire   
Source dâ€™information :  
les rÃ©sultats dâ€™Ã©tudes prÃ©cÃ©dentes supposÃ©es similaires  
les avis de p experts et la proba reprÃ©sente la confiance accordÃ©e Ã  chaque expert. Î˜={Î¸1,...,Î¸p}Ï€1,...,Ï€pP(Î¸=Î¸i)=Ï€i
La loi a posteriori est aussi une loi discrÃ¨te Ã  valeurs dans    
Pour tout i=1â€¦p,  on a   
Si   avec    alors la loi a posteriori ne se concentre pas autour de la vraie valeur câ€™est Ã  dire  il existe  un intervalle U  tel que   et  Î˜P(Î¸=Î¸i|X)=fn(X|Î¸i)Ï€iâˆ‘pj=1Ï€jfn(X|Î¸j)Xâˆ¼PÎ¸0P(Î¸=Î¸0)=0Î¸0âˆˆUP(Î¸âˆˆU|X)â†›1
 la loi a priori est uniforme sur {0, 1/2, 1 ,3/2, 2}  
 les observations sont simulÃ©es suivant la loi exponentielle de paramÃ¨tre 0.75  
Illustration 
On reprÃ©sente la loi a posteriori en fonction de n pour 3 Ã©chantillons.    Evolution des probabilitÃ©s  a posteriori en fonction de n 
les observations sont simulÃ©es suivant la loi exponentielle de paramÃ¨tre 1  
On a    
On observe que     P(Î¸=1)=1/5P(Î¸=1|X)â†’1Illustration (cont.) Evolution des probabilitÃ©s  a posteriori en fonction de n 
 Histogramme
On relaxe le contrainte de finitude de  en prenant comme support un intervalle  
On ordonne les valeurs    
on ajoute une borne infÃ©rieure :       
On construit lâ€™histogramme    
Cette loi  vÃ©rifie  
 pour tout  Î˜={Î¸1,...,Î¸p}Î¸i,i=1,...,pÎ¸0<Î¸1<Î¸2<â‹¯<Î¸pÏ€(Î¸)=pâˆ‘i=1Ï€iÎ¸iâˆ’Î¸iâˆ’11[Î¸iâˆ’1,Î¸i[(Î¸)P(Î¸âˆˆ[Î¸iâˆ’1,Î¸i[)=Ï€iÏ€(Î¸)>0Î¸âˆˆ[Î¸0;Î¸p[
La loi a posteriori sâ€™Ã©crit   Dâ€™oÃ¹  Ï€(Î¸|X1,...Xn)âˆpâˆ‘i=1Ï€iÎ¸iâˆ’Î¸iâˆ’1f(X1,...Xn|Î¸)1[Î¸iâˆ’1,Î¸i[(Î¸)Ï€(Î¸|X1,...Xn)=âˆ‘pi=1Ï€iÎ¸iâˆ’Î¸iâˆ’1f(X1,...Xn|Î¸)1[Î¸iâˆ’1,Î¸i[(Î¸)âˆ‘pi=1Ï€iÎ¸iâˆ’Î¸iâˆ’1âˆ«Î¸iÎ¸iâˆ’1f(X1,...Xn|Î¸)dÎ¸
Illustration (cont.) 
les observations sont simulÃ©es suivant la loi exponentielle de paramÃ¨tre .75.  
A priori uniforme
A priori non uniforme
Famille de lois conjuguÃ©es 
On considÃ¨re   une famille paramÃ©trique de lois sur . 
DÃ©finition  On dit que  est une famille conjuguÃ©e avec  si la loi a priori appartient Ã   alors la loi a posteriori appartient aussi Ã  .  Autrement dit  ğ’«={Ï€Î»,Î»âˆˆÎ›}Î˜ğ’«â„±={fn(â‹…|Î¸),Î¸âˆˆÎ˜}ğ’«ğ’«âˆ€Î»âˆˆÎ›Â siÂ Î¸âˆ¼Ï€Î»Â alorsÂ âˆƒÎ»(X)âˆˆÎ›telÂ queÂ Ï€(Î¸|X)=Ï€Î»(X)(Î¸) Exemple de Famille de lois conjuguÃ©es 
 est la famille des lois exponentielles   
La famille des lois Gamma est une famille de lois conjuguÃ©es :   a> 0 et b> 0.  On a   
 est la famille des lois uniformes sur   avec  
La famille des lois de pareto est une famille de lois conjuguÃ©es :    avec a> et b>0.   On a  â„±f(x1,...,xn|Î¸)=Î¸neâˆ’Î¸nÂ¯XnÏ€(Î¸)=ba1Î“(a)eâˆ’bÎ¸Î¸aâˆ’11Î¸>0Î»=(a,b)âŸ¶Î»(X)=(n+a,b+nÂ¯Xn)â„±[0,Î¸]f(x1,...,xn|Î¸)=1Î¸n1Mnâ‰¤Î¸Mn=max(X,...,Xn)Ï€(Î¸)=abaÎ¸a+11Î¸>bÎ»=(a,b)âŸ¶Î»(X)=(n+a,max(b,Mn))
ğŸ“
ğŸ“
on fixe la valeur de  Ã  partir de lâ€™information disponible a  priori  
Exemple 1. Information a priori :  est autour de 1 
On choisit  tel que .  En fonction de la dimension de  on pourra aussi  ajouter une contrainte sur la variance  de  qui traduit la confiance accordÃ©e Ã  lâ€™information 
Exemple 2. Information a priori :     (avec une forte probabilitÃ©) 
On fixe  tel que  ou 80%, 99% â€¦  en fonction de la confiance accordÃ©e Ã  lâ€™information Î»Î¸Î»E(Î¸)=âˆ«Ï‘Ï€Î»(Ï‘)dÏ‘=1Î›Î¸Î¸âˆˆAÎ»P(Î¸âˆˆA)=âˆ«AÏ€Î»(Ï‘)dÏ‘=95%Choix de lâ€™hyperparamÃ¨tre Î»
MÃ©lange dâ€™experts a priori  
Proposition : Soit    une  famille de lois conjuguÃ©es,  la famille des mÃ©langes de lois de    forme aussi une famille de lois conjuguÃ©es   Rappel  : un mÃ©lange sâ€™Ã©crit   avec     ,  et   
Application : on peut prendre en compte diffÃ©rentes sources dâ€™information et accorder des poids diffÃ©rents en fonction de la fiabilitÃ© des sources ğ’«={Ï€Î»,Î»âˆˆÎ›}ğ’«kâˆ‘j=1pjÏ€Î»j(Î¸)(Î»1,...,Î»k)âˆˆÎ›k(p1,...,pk)âˆˆ[0,1]kkâˆ‘j=1pj=1
Illustration  : modÃ¨le exponentielExpert 1 :    A priori  A posteriori Î¸=1Â±.5ab=1ab2=.52Î“(4,4)Î“(n+4,nÂ¯Xn+4)Expert 2 :    A priori  A posteriori Î¸=2Â±.5ab=2ab2=.52Î“(16,8)Î“(n+16,nÂ¯Xn+8)
A priori A posteriori n= 10MÃ©lange  1/2 -1/2 Lois non informatives  
Loi a priori impropre
On considÃ¨re    telle que  
 ne dÃ©finit pas une loi de probabilitÃ© sur  
DÃ©finition  on dit que  est une loi impropre pour le modÃ¨le  si   
Si  est une loi impropre alors la loi a posteriori est bien dÃ©finie par  Ï€:Î˜â†¦R+{âˆ‘Ï‘âˆˆÎ˜Ï€(Ï‘)=âˆÂ discrÃ¨teÂ âˆ«Î˜Ï€(Ï‘)dÏ‘=âˆÂ continueÂ Ï€Î˜Ï€:Î˜â†¦R+â„±={fn(â‹…|Î¸),Î¸âˆˆÎ˜}âˆ«Î˜Ï€(Î¸)fn(x1,...xn|Î¸)dÎ¸<âˆpresqueÂ surement.Ï€Ï€(Î¸|X)=Ï€(Î¸)fn(X|Î¸)âˆ«Î˜Ï€(Ï‘)fn(X|Ï‘)dÏ‘
Si 
 est une loi impropre alors pour tout C>0,   
 est aussi une loi impropre. 
A partir de ces  deux lois impropres, on obtient la mÃªme loi a posteriori
Ï€
Î½
(
Î¸
)
=
C
Ï€
(
Î¸
)
Exemple : modÃ¨le exponentiel
On considÃ¨re     
on a    
La fonction   dÃ©finit donc une loi impropre si  et seulement n>1  
Pour n> 1,  la loi a posteriori est la loi gamma   Ï€(Î¸)=1R+(Î¸)âˆ«R+Ï€(Ï‘)dÏ‘=âˆâˆ«Î˜Ï€(Ï‘)fn(x1,...xn|Ï‘)dÏ‘=âˆ«R+Ï‘neâˆ’Ï‘nÂ¯XndÏ‘<âˆâ‡”(n>1Â etÂ Â¯Xn>0).Ï€Î“(n+1,nÂ¯Xn)
Loi a priori de Laplace 
Si  est un ensemble fini ou de mesure de Lebesgue finie () alors la loi  a priori de Laplace est la loi uniforme sur   
Si   ou      alors la loi a priori de Laplace est une loi impropre dÃ©finie par   .  
Proposition : Si  la loi a priori de Laplace existe  alors la loi a posteriori vÃ©rifie  Î˜âˆ«Î˜dÏ‘<âˆÎ˜{Î˜Â infiniÂ dÃ©nombrableÂ âˆ‘Ï‘âˆˆÎ˜fn(X|Ï‘)<âˆÂ âˆ«Î˜dÏ‘=âˆÂ âˆ«Î˜fn(X|Ï‘)dÏ‘<âˆÂ Ï€(Î¸)âˆ1Î˜(Î¸)Ï€(Î¸|X)âˆfn(X|Î¸) Loi Non informative de Jeffreys
On suppose que le modÃ¨le  est rÃ©gulier  
Soit  lâ€™information de Fisher  et son dÃ©terminant  
Si     ou   si   et   alors la loi de Jeffreys est dÃ©finie par  â„±={fn(â‹…|Î¸),Î¸âˆˆÎ˜}In|In|âˆ«Î˜|In(Ï‘)|dÏ‘<âˆâˆ«Î˜|In(Ï‘)|)dÏ‘=âˆâˆ«Î˜|In(Ï‘)|fn(X|Ï‘)dÏ‘<âˆÏ€(Ï‘)âˆ|In(Ï‘)|
Loi Non informative de Jeffreys
La loi de Jeffreys favorise les rÃ©gions oÃ¹ lâ€™information de Fisher prend des grandes valeurs câ€™est Ã  dire les rÃ©gions oÃ¹ les donnÃ©es apportent plus dâ€™information sur le paramÃ¨tre  
La loi de Jeffreys est invariante par reparamÃ©trisation  
Exemple : modÃ¨le exponentiel 
On considÃ¨re  n variables alÃ©atoires iid suivant la loi exponentielle  
Lâ€™information de Fisher est donnÃ©e par    
 et  ps car n>0 et  ps  
La loi de Jeffreys est une loi impropre dÃ©finie   
La loi a posteriori est la loi gamma nÎ¸2âˆ«âˆO1Ï‘dÏ‘=âˆâˆ«âˆOÏ‘nâˆ’1eâˆ’nÎ¸Â¯XndÏ‘<âˆÂ¯Xn>0Ï€(Î¸)âˆ1Î¸1R+(Î¸)Î“(n,nÂ¯Xn)
ğŸ“